[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.8.0","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","projets",["Map",9,10,50,51,79,80,109,110,140,141,171,172,201,202,231,232,259,260,289,290],"egalite-cinema-festivals",{"id":9,"data":11,"filePath":40,"digest":41,"rendered":42,"legacyId":49},{"title":12,"slug":9,"images":13,"description":19,"details":20,"role":21,"durée":22,"technos":28,"team":39},"Analyse des statistiques d'égalité dans le cinéma et les festivals",[14,15,16,17,18],"/projet5050/archi1.png","/projet5050/bdd.png","/projet5050/front5050.png","/projet5050/metabase.png","/projet5050/ml.png","Un projet d’envergure lancé à l’occasion du Festival de Cannes, visant à centraliser, analyser et visualiser les données liées aux inégalités dans le cinéma et les festivals. De la répartition des budgets aux nominations, en passant par l’analyse des affiches et trailers, cette plateforme met en lumière les disparités du secteur.\n","Le projet s’est articulé autour de la collecte massive de données issues de sources publiques et privées grâce à un travail approfondi de scraping, de nettoyage et d’organisation dans une base PostgreSQL. Une interface web en React/Next.js permet d’explorer les statistiques, tandis que Metabase facilite l’analyse dynamique pour les utilisateurs.  \nUn sous-système a également été développé pour automatiser l’analyse des visuels (affiches) et trailers, avec des outputs variés sur la représentation selon le genre, l’origine, et d’autres critères. Ces modules d’analyse sont progressivement automatisés pour une intégration directe dans la base.\n\nLe déploiement a été pensé pour être scalable et robuste. L’architecture a été mise en place avec une approche DevOps complète, incluant l’automatisation via Terraform, la gestion des workflows avec Airflow, et la surveillance avec Prometheus et Grafana.\n\nUne attention particulière a été portée à l’intégration continue, au versioning des données, et à la traçabilité pour garantir la fiabilité des analyses diffusées, notamment à destination de partenaires comme France Télévisions.\n","Responsable technique, en charge de l’architecture, du déploiement, et de l’intégration globale des composants data et web",[23,24,25,26,27],"Phase 1 (2 mois) Compréhension du besoin et conception d’une architecture data adaptée au budget et aux ressources humaines disponibles.","Phase 2 (3 mois) Mise en place de l’architecture technique (frontend, base de données, Metabase, backend) et entraînement des modèles de machine learning pour l’analyse d’images et de vidéos. Travail sur la qualité et l’exploitation des outputs.","Phase 3 (1 mois) Déploiement de la base de données, du frontend et du backend, avec intégration d’une chaîne CI/CD.","Phase 4 (en cours) Déploiement et intégration des données issues du machine learning dans la base de données, en vue d’analyses avancées.","Phase 5 (à venir) Automatisation du scrapping et des inférences du modèle de machine learning.",[29,30,31,32,33,34,35,36,37,38],"Python","React","Next.js","PostgreSQL","Metabase","Airflow","Grafana","Docker","Web Scraping","Analyse d’image et vidéo (affiches et trailers)","Une équipe pluridisciplinaire composée de : - 1 développeur front-end - 3 développeurs backend/data - 3 data scientists - 2 data analysts spécialisés en ML - 1 cheffe de projet Le tout en collaboration étroite avec l’association à l’origine de l’initiative, pour proposer une plateforme utile, accessible, et en phase avec les enjeux sociétaux actuels.\n","src/content/projets/c5050.md","41e7439fcba960a4",{"html":43,"metadata":44},"",{"headings":45,"localImagePaths":46,"remoteImagePaths":47,"frontmatter":11,"imagePaths":48},[],[],[],[],"c5050.md","conso-eau",{"id":50,"data":52,"filePath":70,"digest":71,"rendered":72,"legacyId":78},{"title":53,"slug":50,"images":54,"description":58,"details":59,"role":60,"durée":61,"technos":65,"team":69},"Application de diagnostic de consommation d’eau",[55,56,57],"/projetEau/site.png","/projetEau/Archi.png","/projetEau/superset.png","Un site interactif conçu pour permettre à chacun d’estimer sa consommation d’eau gratuitement, en répondant à une série de questions précises. L’outil fournit un retour personnalisé, basé sur des calculs détaillés.\n","Nous avons d’abord imaginé une expérience utilisateur simple et fluide avec l’aide d’un UX/UI designer. Le cœur du projet repose sur un moteur de calcul développé en Python, exposé via une API hébergée sur un VPS optimisé pour les faibles coûts.\n\nLe front-end a été développé en React avec Next.js, en intégrant des visualisations grâce à Superset pour l’analyse en temps réel des réponses. PostgreSQL a été utilisé pour la persistance des données. La mise en production a été réalisée avec une architecture robuste, flexible et scalable.\n\nCe projet nous a permis de tester un POC complet et fonctionnel, avec un cycle de calcul régulier optimisé pour la montée en charge.\n","Tech lead, architecture data, déploiement backend et frontend",[62,63,64],"Phase 1 (1 mois) Compréhension des besoins et proposition d’une architecture adaptée, en tenant compte du budget et des moyens humains disponibles.","Phase 2 (3 mois) Mise en place de l’architecture technique : base de données, backend, Superset et frontend.","Phase 3 (1 mois)Passage et déploiement sur la VPS, avec une phase de test et réalisation d’un POC (Proof of Concept).",[32,66,30,31,29,67,68],"Superset","API REST","VPS (Debian)","Projet mené avec une équipe composée de 2 data scientists, un développeur front-end junior (accompagné techniquement), et un UX/UI designer. Une forte collaboration entre profils techniques et design pour un résultat cohérent.\n","src/content/projets/conso-eau.md","2b80d7f5d5f23446",{"html":43,"metadata":73},{"headings":74,"localImagePaths":75,"remoteImagePaths":76,"frontmatter":52,"imagePaths":77},[],[],[],[],"conso-eau.md","detection-oiseaux",{"id":79,"data":81,"filePath":100,"digest":101,"rendered":102,"legacyId":108},{"title":82,"slug":79,"images":83,"description":87,"details":88,"durée":89,"role":93,"technos":94,"team":99},"Détection sonore automatisée pour la migration des oiseaux",[84,85,86],"/projetNBM/StructureNBM.png","/projetNBM/db_NBM.png","/projetNBM/CDC_NBM.png","Dans le cadre d’une collaboration avec une association d’ornithologues, nous avons conçu un système pour détecter et classifier les chants d’oiseaux migrateurs durant la nuit. L’objectif ? Aider les experts à mieux suivre les flux migratoires, en automatisant la reconnaissance sonore pour chaque espèce.\n","Le défi était double : entraîner un modèle de machine learning performant sur des sons complexes et variables, et déployer une architecture robuste pour le traitement continu des enregistrements.Nous avons mis en place une chaîne complète d’automatisation pour que le modèle s’améliore en continu, à partir de nouvelles données collectées chaque nuit.Ce projet a été une vraie aventure collective, mêlant passion pour la nature, exigence scientifique et mise en œuvre technique rigoureuse.\n",[90,91,92],"Phase 1 (2 mois)  Compréhension du fonctionnement du modèle de machine learning (ML) et de la stack technique existante.","Phase 2 (2 mois)  Optimisation du code pour faciliter le déploiement et l’automatisation du modèle ML. Choix d’une architecture adaptée aux besoins spécifiques de l’association.","Phase 3 (2 mois)  Déploiement de la solution et automatisation des processus.","Développeur MLops",[95,32,96,97,36,98],"MinIO / S3","FastAPI","RabbitMQ","Apache Airflow","Réalisé avec une équipe pluridisciplinaire :  1 data scientists,3 MLops, ornithologues, dans une dynamique collaborative.\n","src/content/projets/detection-oiseaux.md","b76e726a7f76a0d9",{"html":43,"metadata":103},{"headings":104,"localImagePaths":105,"remoteImagePaths":106,"frontmatter":81,"imagePaths":107},[],[],[],[],"detection-oiseaux.md","fiches-environnement-fp",{"id":109,"data":111,"filePath":131,"digest":132,"rendered":133,"legacyId":139},{"title":112,"slug":109,"images":113,"description":116,"details":117,"durée":118,"role":122,"technos":123,"team":130},"Fiches pratiques environnementales pour la fonction publique",[114,115],"/projetFP/archiFP.png","/projetFP/ficheRH.png","Un projet pilote conçu pour accompagner les agents de la fonction publique dans l'intégration de pratiques plus respectueuses de l'environnement à travers des fiches pratiques accessibles. L'objectif est de fournir un support concret, évolutif, et co-construit par les utilisateurs, dans une logique de sobriété numérique.\n","Après une phase d'exploration des outils existants (Suite Numérique, Tchap), nous avons opté pour la mise en place d’un wiki frugal, centré sur la production collaborative de fiches pratiques.\n\nLe cœur du dispositif repose sur deux systèmes RAG (Retrieval-Augmented Generation) expérimentés en parallèle, avec des contenus provenant d'une API développée avec WeLearn, de la littérature grise, et de contributions internes. Les utilisateurs peuvent enrichir les fiches, proposer des modifications, et manifester leur intérêt via un formulaire.\n\nEn cas de convergence sur une thématique, un système de proposition automatisée de visio est déclenché pour favoriser l’échange collectif. Un retraining progressif permet d’améliorer la pertinence des fiches au fil du temps.\n",[119,120,121],"Phase 1 (2 mois) Compréhension et analyse des besoins.","Phase 2 (4 mois) Choix de la stack technologique et amélioration du modèle.","Phase 3 (1 mois) Déploiement et automatisation de la solution.","Responsable technique du POC, intégration des systèmes RAG, API WeLearn et mise en place du pipeline d’amélioration continue",[124,125,126,29,32,127,128,129],"Wiki frugal (statique)","RAG (x2 moteurs comparés)","API WeLearn","Automatisation email/visio","UX Research (entretiens utilisateurs)","UI Design collaboratif","Projet mené en collaboration avec des responsables RH de la fonction publique issus de plusieurs territoires, des experts en UX/UI, et l'équipe technique de WeLearn. L’approche a été fortement centrée utilisateur, avec des ateliers et des tests continus.\n","src/content/projets/ecoskills.md","7caf27942ff16bef",{"html":43,"metadata":134},{"headings":135,"localImagePaths":136,"remoteImagePaths":137,"frontmatter":111,"imagePaths":138},[],[],[],[],"ecoskills.md","detection-defaillance-photovoltaique",{"id":140,"data":142,"filePath":162,"digest":163,"rendered":164,"legacyId":170},{"title":143,"slug":140,"images":144,"description":149,"details":150,"durée":151,"role":155,"technos":156,"team":161},"Système de détection de défaillance sur fermes photovoltaïques flottantes",[145,146,147,148],"/projetPV/analyse.png","/projetPV/pv2.png","/projetPV/roadmap.png","/projetPV/pvschema.png","Une solution innovante de détection automatique de défaillances sur des fermes de panneaux photovoltaïques flottants sur des lacs. Le système combine IoT, analyse de données et machine learning pour optimiser la maintenance et maximiser la production.\n","Ce projet a débuté par une analyse approfondie des données collectées via des capteurs IoT installés sur la centrale EDF. Avec une équipe composée d’un développeur et de deux ingénieurs, nous avons mis en place des méthodes de data science pour identifier et optimiser les informations pertinentes pour la détection d’anomalies.\n\nLa réflexion s’est concentrée sur l’obtention des insights les plus fiables avec le meilleur rapport qualité/prix. Ensuite, une solution automatisée a été développée, incluant un modèle de machine learning déployé pour détecter en temps réel les défaillances.\n\nLe déploiement a été effectué en interne avec une architecture robuste utilisant Terraform pour l’infrastructure, Airflow pour l’orchestration des workflows, et PostgreSQL pour la gestion des données.\n",[152,153,154],"Phase 1 (6 mois) Analyse des besoins et étude de la viabilité du modèle de détection d’anomalies.","Phase 2 (en cours) Déploiement des capteurs IoT sur site.","Phase 3 (à venir) Entraînement et déploiement du modèle de détection, avec mise en place de l’automatisation.","Data Scientist et MLOps, responsable des modèles ML et du déploiement automatisé",[29,157,35,158,34,159,32,160],"Prometheus","Clustering","Terraform","IoT","Projet mené avec un développeur et deux ingénieurs, en forte collaboration autour de la collecte IoT, du développement ML et de l’intégration opérationnelle sur la centrale EDF.\n","src/content/projets/helioslite.md","840588696cdd54c7",{"html":43,"metadata":165},{"headings":166,"localImagePaths":167,"remoteImagePaths":168,"frontmatter":142,"imagePaths":169},[],[],[],[],"helioslite.md","classification-clients-huissiers",{"id":171,"data":173,"filePath":192,"digest":193,"rendered":194,"legacyId":200},{"title":174,"slug":171,"images":175,"description":178,"details":179,"durée":180,"role":185,"technos":186,"team":191},"Classification de clients pour optimisation des relances téléphoniques",[176,177],"/huissiers/archi.png","/huissiers/bdd.png","Projet de data science et machine learning réalisé pour un groupe d’huissiers afin de prédire la probabilité de remboursement de créanciers. L’objectif était d’optimiser les appels de relance en priorisant les clients selon leur potentiel de paiement.\n","Le projet a débuté par une analyse exploratoire approfondie des bases de données clients transmises par les huissiers. Cette étape a permis de comprendre la structure, la variabilité et la véracité des informations disponibles. Un travail rigoureux de nettoyage a été effectué, notamment via la mise en place d’un pipeline ELT pour fiabiliser les données entrantes.\nEn collaboration avec l’équipe SI (2 personnes) et l’équipe administrative en charge des appels, nous avons identifié les critères influençant la capacité de remboursement. Sur cette base, plusieurs itérations de modèles de classification ont été entraînées et évaluées, en intégrant progressivement le retour terrain pour affiner les prédictions.\nUn système de scoring a été mis en place pour prioriser les relances, avec des tests réguliers d’amélioration des performances. Enfin, la solution a été déployée et automatisée, permettant une classification continue et une meilleure efficacité des équipes de relance.\n",[181,182,183,184],"Phase 1 (2 mois) : Analyse des tables, vérification de la qualité des données et mise en place d’un pipeline ELT pour le nettoyage initial.","Phase 2 (2 mois) : Collaboration avec l’équipe métier pour identifier les critères de scoring, exploration statistique et construction des premières features.","Phase 3 (2 mois) : Entraînement et amélioration de modèles de classification, tests croisés avec retours de l’équipe administrative.","Phase 4 (2 mois) : Déploiement du modèle, automatisation des flux de données et intégration dans le système de relance.","Arhictecte data, responsable du pipeline ELT, de la modélisation prédictive et de l’automatisation de la solution",[29,187,188,34,32,96,189,190],"Scikit-learn","Pandas","ELT","Classification supervisée","Projet mené avec l’équipe SI (2 personnes) et l’équipe administrative des huissiers. Collaboration étroite pour le recueil des besoins métiers, la validation des résultats et l’intégration dans le processus opérationnel de relance téléphonique.\n","src/content/projets/huissiers.md","98f74b1cab2b48d0",{"html":43,"metadata":195},{"headings":196,"localImagePaths":197,"remoteImagePaths":198,"frontmatter":173,"imagePaths":199},[],[],[],[],"huissiers.md","classification-retours-techniques",{"id":201,"data":203,"filePath":222,"digest":223,"rendered":224,"legacyId":230},{"title":204,"slug":201,"images":205,"description":208,"details":209,"durée":210,"role":215,"technos":216,"team":221},"Classification intelligente des retours techniques sur distributeurs de médicaments",[206,207],"/projetRetours/architecture.png","/projetRetours/dashboard.png","Mise en place d’un système de classification automatique des retours techniques issus des distributeurs automatiques de médicaments. Grâce à des techniques de machine learning non supervisé et à un dashboard Power BI, l’entreprise peut désormais prioriser ses interventions et améliorer sa maintenance préventive.\n","L’entreprise installe et maintient des machines de distribution de médicaments en pharmacie. Face à une volumétrie croissante de retours techniques transmis par les techniciens, l’objectif a été de structurer ces données pour en extraire de la valeur.\nLe projet a commencé par une collaboration étroite avec l’équipe technique pour définir les grandes **catégories de pannes** et **mots-clés caractéristiques**. Ces éléments ont ensuite guidé une phase de **clustering non supervisé** en deep learning sur des retours non labellisés, permettant de faire émerger des regroupements cohérents.\nUne fois la classification validée et enrichie en itération avec l’équipe technique, elle a été **déployée directement dans la base de données** centralisant les incidents. Enfin, un **dashboard interactif sous Power BI** a été développé pour permettre à la direction et aux équipes opérationnelles de visualiser les typologies de pannes, les fréquences, et les tendances par pharmacie, par machine, ou par période.\n",[211,212,213,214],"Phase 1 (1 mois) Définition des catégories et mots-clés avec l’équipe technique","Phase 2 (1,5 mois) Clustering non supervisé sur les tickets techniques","Phase 3 (1 mois) Déploiement de la classification sur la base de données","Phase 4 (0,5 mois) Développement et mise en production du dashboard Power BI","Architecte Data  – responsable du traitement NLP, du clustering et du reporting analytique",[29,217,218,219,220,32],"scikit-learn","TensorFlow","NLP","Power BI","Projet mené en collaboration avec l’équipe technique (techniciens de maintenance), l’équipe administrative et la direction de l’entreprise. Une approche collaborative a permis d’aligner les catégories métier avec les données réelles et de favoriser l’adoption des outils analytiques.\n","src/content/projets/synergie.md","575f7a39068ef7ca",{"html":43,"metadata":225},{"headings":226,"localImagePaths":227,"remoteImagePaths":228,"frontmatter":203,"imagePaths":229},[],[],[],[],"synergie.md","nounou-app-assmat",{"id":231,"data":233,"filePath":250,"digest":251,"rendered":252,"legacyId":258},{"title":234,"slug":231,"images":235,"description":238,"details":239,"role":240,"durée":241,"technos":243,"team":249},"Nounou App – Application de suivi pour assistantes maternelles",[236,237],"/projetnounou/nounou.png","/projetnounou/archi base.png","Une application web pensée pour simplifier le quotidien des assistantes maternelles : suivi des enfants, communication avec les parents, génération automatique des récapitulatifs mensuels pour Pajemploi. Conçue pour être simple, sécurisée et accessible, même sans compétences numériques avancées.\n","Projet mené de A à Z, en autonomie complète, avec un focus sur l’impact concret et l’utilisabilité terrain. L’app permet aux assistantes maternelles de :\n- Suivre les soins, repas, siestes et activités des enfants. - Partager en toute sécurité des photos, notes et documents avec les parents. - Gérer plusieurs profils d’enfants avec historique et journal de bord personnalisé. - Générer automatiquement des récapitulatifs mensuels de présence, prêts à être utilisés pour Pajemploi. - Bénéficier d’un espace personnel sécurisé grâce à une authentification utilisateur.\nLe front-end a été développé avec **React + Vite** et déployé sur **Vercel**. Les données sont stockées dans une base **PostgreSQL hébergée sur Supabase**, qui gère également l’authentification et le stockage des fichiers (photos, documents partagés).\nPlusieurs retours d’utilisatrices terrain (assistantes maternelles) ont été intégrés tout au long du développement pour affiner l’ergonomie, les fonctionnalités et la terminologie utilisée.\n","Développeur full-stack indépendant. En charge de toutes les étapes : définition du besoin, développement front et back, déploiement, tests avec utilisatrices, ajustements UX/UI, gestion des performances et de la sécurité.\n",[242],"3 mois – Développement itératif avec retours hebdomadaires d’utilisatrices. App construite en full remote, testée dans des contextes réels.",[30,244,245,246,247,248],"Vite","Supabase (PostgreSQL, Auth, Storage)","JavaScript","HTML/CSS","Vercel","Projet solo mené en autonomie. Retours réguliers de 2 assistantes maternelles pour affiner les fonctionnalités selon les usages réels.\n","src/content/projets/assmatapp.md","144e1768965a9d15",{"html":43,"metadata":253},{"headings":254,"localImagePaths":255,"remoteImagePaths":256,"frontmatter":233,"imagePaths":257},[],[],[],[],"assmatapp.md","applications-education-nationale",{"id":259,"data":261,"filePath":280,"digest":281,"rendered":282,"legacyId":288},{"title":262,"slug":259,"images":263,"description":266,"details":267,"role":268,"durée":269,"technos":272,"team":279},"Applications éducatives et inclusives pour l’Éducation Nationale",[264,265],"/projeteducnat/dys.png","/projeteducnat/chatbot.png","Des outils numériques légers et concrets développés pour répondre aux besoins du terrain dans l'Éducation Nationale : favoriser l'inclusion des élèves à besoins éducatifs particuliers (EBEP), et rassurer les familles grâce à des assistants intelligents, accessibles et sécurisés.\n","Ces projets ont été réalisés de manière autonome, en dialogue étroit avec des acteurs du système éducatif (enseignants spécialisés, référents ASH, familles). Objectif : proposer des solutions gratuites, simples à utiliser et déployables rapidement sur le terrain éducatif.\n**Outil d’accessibilité pour élèves EBEP**   Webapp développée à la demande du **service ASH du rectorat de Lyon**, permettant de : - Transformer un fichier PDF en **version audio** (synthèse vocale) - Convertir de l’audio en **texte lisible ou sous-titré** - Traduire automatiquement un document dans plusieurs langues - Générer une version \"adaptée DYS\" (police, mise en page, espacement)\nL’application vise à rendre les supports pédagogiques plus accessibles aux élèves en situation de handicap ou allophones, sans dépendre de logiciels complexes ou payants.\n**Chatbot éducatif pour les parents d’élèves à besoins particuliers**   Assistant conversationnel basé sur un système **RAG (Retrieval-Augmented Generation)**, permettant aux parents : - d'obtenir des réponses fiables et contextualisées (ressources officielles de l’Éducation Nationale) - de recevoir des **conseils pratiques** adaptés à la situation de leur enfant - de mieux comprendre les dispositifs d’accompagnement scolaire (PPS, PAP, PAI, AESH, etc.)\nL’interface est pensée pour un usage non-technique, avec un langage clair, une navigation guidée et une dimension rassurante dans la formulation des réponses.\n","Développeur full-stack indépendant. Conception, développement, intégration de modèles de langage, déploiement. Projets réalisés avec une approche centrée \"utilisateur vulnérable\", dans un cadre non-marchand.\n",[270,271],"Accessibilité EBEP – 1,5 mois : développement d’une webapp en lien avec un enseignant référent ASH et testée dans deux établissements de la métropole lyonnaise.","Chatbot éducatif – 2 mois : création d’un assistant intelligent avec système RAG, entraîné sur des documents Éducation Nationale et testé auprès de parents d’élèves avec AESH.",[29,273,96,274,275,276,277,247,278],"Streamlit","LangChain","Hugging Face Transformers","Whisper / TTS","PDF parsing","SQLite","Projets réalisés en solo, avec accompagnement de professionnel·les de l’éducation spécialisée (enseignant·es ASH, parents d’élèves, référents de circonscription).\n","src/content/projets/projetseducnat.md","c5fc6b8cd9111a07",{"html":43,"metadata":283},{"headings":284,"localImagePaths":285,"remoteImagePaths":286,"frontmatter":261,"imagePaths":287},[],[],[],[],"projetseducnat.md","communication-non-violente-parentalite",{"id":289,"data":291,"filePath":306,"digest":307,"rendered":308,"legacyId":314},{"title":292,"slug":289,"images":293,"description":296,"details":297,"role":298,"durée":299,"technos":301,"team":305},"Application d’aide à la communication parentale post-séparation",[294,295],"/projettiers/tiers.png","/projettiers/Workflow.png","Un outil numérique visant à limiter les violences verbales dans les échanges écrits entre parents séparés, notamment lors de la gestion partagée des enfants. L’application propose une classification automatique des messages à risque, un adoucissement linguistique des propos, ainsi qu’un tableau de bord d’analyse des types de violences verbales recensées.\n","Ce projet a été conçu comme une solution légère et accessible, destinée aux familles en situation de tension, aux médiateurs familiaux ou aux services sociaux. Il repose sur trois piliers techniques :\n**1. Classification automatique des messages violents**   Un pipeline de traitement permet de détecter et classifier les types de violence verbale dans un message (insultes, menaces, culpabilisation, chantage, ironie passive, etc.) à l’aide d’un modèle supervisé basé sur **Random Forest** et complété par une analyse lexicale.   Un corpus annoté de messages anonymisés a été constitué pour l'entraînement, en incluant des formulations typiques de SMS parentaux en contexte de séparation.\n**2. Générateur de reformulation en langage neutre (RAG spécialisé)**   Utilisation d’un système RAG (Retrieval-Augmented Generation) adapté au style **SMS/courts messages**, capable de reformuler un message tendu de façon plus factuelle et apaisée. Le système conserve le contenu informationnel tout en retirant les éléments agressifs ou accusateurs. Il est également capable de suggérer des formulations alternatives plus acceptables.\n**3. Dashboard de suivi statistique (Apache Superset)**   Mise en place d’un tableau de bord interactif permettant : - d’analyser la fréquence des types de violence détectés - de visualiser l’évolution des tensions dans les échanges - de fournir des indicateurs utilisables par des médiateurs ou travailleurs sociaux\nCe dashboard vise à objectiver les dynamiques de communication parentale et à mieux cibler les besoins d’accompagnement.\n","Développeur full-stack et data scientist indépendant. Conception du projet, constitution du jeu de données, développement des modèles de classification et de génération, mise en place du front et du dashboard. Travail réalisé dans une optique sociale, avec attention à la faisabilité low-cost et au respect des données sensibles.\n",[300],"2 mois : création du dataset, entraînement du modèle de classification, mise en place du système RAG personnalisé, développement d’un mini-front d’édition, et intégration d’un dashboard analytique Superset.",[29,302,275,96,303,278,304],"Scikit-learn (Random Forest)","Apache Superset","Streamlit (version test rapide)","Projet réalisé en autonomie, avec consultation de médiateurs familiaux et de psychologues spécialisés en communication non-violente. Approche éthique : données anonymisées, attention au consentement, et transparence du fonctionnement du modèle.\n","src/content/projets/tiers.md","0144c7166bb0bb05",{"html":43,"metadata":309},{"headings":310,"localImagePaths":311,"remoteImagePaths":312,"frontmatter":291,"imagePaths":313},[],[],[],[],"tiers.md"]