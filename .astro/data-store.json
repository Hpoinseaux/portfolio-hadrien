[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.8.0","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","projets",["Map",9,10,50,51,79,80,109,110,140,141,171,172,207,208,237,238],"egalite-cinema-festivals",{"id":9,"data":11,"filePath":40,"digest":41,"rendered":42,"legacyId":49},{"title":12,"slug":9,"images":13,"description":19,"details":20,"role":21,"durée":22,"technos":28,"team":39},"Analyse des statistiques d'égalité dans le cinéma et les festivals",[14,15,16,17,18],"/projet5050/archi1.png","/projet5050/bdd.png","/projet5050/front5050.png","/projet5050/metabase.png","/projet5050/ml.png","Un projet d’envergure lancé à l’occasion du Festival de Cannes, visant à centraliser, analyser et visualiser les données liées aux inégalités dans le cinéma et les festivals. De la répartition des budgets aux nominations, en passant par l’analyse des affiches et trailers, cette plateforme met en lumière les disparités du secteur.\n","Le projet s’est articulé autour de la collecte massive de données issues de sources publiques et privées grâce à un travail approfondi de scraping, de nettoyage et d’organisation dans une base PostgreSQL. Une interface web en React/Next.js permet d’explorer les statistiques, tandis que Metabase facilite l’analyse dynamique pour les utilisateurs.  \nUn sous-système a également été développé pour automatiser l’analyse des visuels (affiches) et trailers, avec des outputs variés sur la représentation selon le genre, l’origine, et d’autres critères. Ces modules d’analyse sont progressivement automatisés pour une intégration directe dans la base.\n\nLe déploiement a été pensé pour être scalable et robuste. L’architecture a été mise en place avec une approche DevOps complète, incluant l’automatisation via Terraform, la gestion des workflows avec Airflow, et la surveillance avec Prometheus et Grafana.\n\nUne attention particulière a été portée à l’intégration continue, au versioning des données, et à la traçabilité pour garantir la fiabilité des analyses diffusées, notamment à destination de partenaires comme France Télévisions.\n","Responsable technique, en charge de l’architecture, du déploiement, et de l’intégration globale des composants data et web",[23,24,25,26,27],"Phase 1 (2 mois) Compréhension du besoin et conception d’une architecture data adaptée au budget et aux ressources humaines disponibles.","Phase 2 (3 mois) Mise en place de l’architecture technique (frontend, base de données, Metabase, backend) et entraînement des modèles de machine learning pour l’analyse d’images et de vidéos. Travail sur la qualité et l’exploitation des outputs.","Phase 3 (1 mois) Déploiement de la base de données, du frontend et du backend, avec intégration d’une chaîne CI/CD.","Phase 4 (en cours) Déploiement et intégration des données issues du machine learning dans la base de données, en vue d’analyses avancées.","Phase 5 (à venir) Automatisation du scrapping et des inférences du modèle de machine learning.",[29,30,31,32,33,34,35,36,37,38],"Python","React","Next.js","PostgreSQL","Metabase","Airflow","Grafana","Docker","Web Scraping","Analyse d’image et vidéo (affiches et trailers)","Une équipe pluridisciplinaire composée de : - 1 développeur front-end - 3 développeurs backend/data - 3 data scientists - 2 data analysts spécialisés en ML - 1 cheffe de projet Le tout en collaboration étroite avec l’association à l’origine de l’initiative, pour proposer une plateforme utile, accessible, et en phase avec les enjeux sociétaux actuels.\n","src/content/projets/c5050.md","41e7439fcba960a4",{"html":43,"metadata":44},"",{"headings":45,"localImagePaths":46,"remoteImagePaths":47,"frontmatter":11,"imagePaths":48},[],[],[],[],"c5050.md","conso-eau",{"id":50,"data":52,"filePath":70,"digest":71,"rendered":72,"legacyId":78},{"title":53,"slug":50,"images":54,"description":58,"details":59,"role":60,"durée":61,"technos":65,"team":69},"Application de diagnostic de consommation d’eau",[55,56,57],"/projetEau/site.png","/projetEau/Archi.png","/projetEau/superset.png","Un site interactif conçu pour permettre à chacun d’estimer sa consommation d’eau gratuitement, en répondant à une série de questions précises. L’outil fournit un retour personnalisé, basé sur des calculs détaillés.\n","Nous avons d’abord imaginé une expérience utilisateur simple et fluide avec l’aide d’un UX/UI designer. Le cœur du projet repose sur un moteur de calcul développé en Python, exposé via une API hébergée sur un VPS optimisé pour les faibles coûts.\n\nLe front-end a été développé en React avec Next.js, en intégrant des visualisations grâce à Superset pour l’analyse en temps réel des réponses. PostgreSQL a été utilisé pour la persistance des données. La mise en production a été réalisée avec une architecture robuste, flexible et scalable.\n\nCe projet nous a permis de tester un POC complet et fonctionnel, avec un cycle de calcul régulier optimisé pour la montée en charge.\n","Tech lead, architecture data, déploiement backend et frontend",[62,63,64],"Phase 1 (1 mois) Compréhension des besoins et proposition d’une architecture adaptée, en tenant compte du budget et des moyens humains disponibles.","Phase 2 (3 mois) Mise en place de l’architecture technique : base de données, backend, Superset et frontend.","Phase 3 (1 mois)Passage et déploiement sur la VPS, avec une phase de test et réalisation d’un POC (Proof of Concept).",[32,66,30,31,29,67,68],"Superset","API REST","VPS (Debian)","Projet mené avec une équipe composée de 2 data scientists, un développeur front-end junior (accompagné techniquement), et un UX/UI designer. Une forte collaboration entre profils techniques et design pour un résultat cohérent.\n","src/content/projets/conso-eau.md","2b80d7f5d5f23446",{"html":43,"metadata":73},{"headings":74,"localImagePaths":75,"remoteImagePaths":76,"frontmatter":52,"imagePaths":77},[],[],[],[],"conso-eau.md","detection-oiseaux",{"id":79,"data":81,"filePath":100,"digest":101,"rendered":102,"legacyId":108},{"title":82,"slug":79,"images":83,"description":87,"details":88,"durée":89,"role":93,"technos":94,"team":99},"Détection sonore automatisée pour la migration des oiseaux",[84,85,86],"/projetNBM/StructureNBM.png","/projetNBM/db_NBM.png","/projetNBM/CDC_NBM.png","Dans le cadre d’une collaboration avec une association d’ornithologues, nous avons conçu un système pour détecter et classifier les chants d’oiseaux migrateurs durant la nuit. L’objectif ? Aider les experts à mieux suivre les flux migratoires, en automatisant la reconnaissance sonore pour chaque espèce.\n","Le défi était double : entraîner un modèle de machine learning performant sur des sons complexes et variables, et déployer une architecture robuste pour le traitement continu des enregistrements.Nous avons mis en place une chaîne complète d’automatisation pour que le modèle s’améliore en continu, à partir de nouvelles données collectées chaque nuit.Ce projet a été une vraie aventure collective, mêlant passion pour la nature, exigence scientifique et mise en œuvre technique rigoureuse.\n",[90,91,92],"Phase 1 (2 mois)  Compréhension du fonctionnement du modèle de machine learning (ML) et de la stack technique existante.","Phase 2 (2 mois)  Optimisation du code pour faciliter le déploiement et l’automatisation du modèle ML. Choix d’une architecture adaptée aux besoins spécifiques de l’association.","Phase 3 (2 mois)  Déploiement de la solution et automatisation des processus.","Développeur MLops",[95,32,96,97,36,98],"MinIO / S3","FastAPI","RabbitMQ","Apache Airflow","Réalisé avec une équipe pluridisciplinaire :  1 data scientists,3 MLops, ornithologues, dans une dynamique collaborative.\n","src/content/projets/detection-oiseaux.md","b76e726a7f76a0d9",{"html":43,"metadata":103},{"headings":104,"localImagePaths":105,"remoteImagePaths":106,"frontmatter":81,"imagePaths":107},[],[],[],[],"detection-oiseaux.md","fiches-environnement-fp",{"id":109,"data":111,"filePath":131,"digest":132,"rendered":133,"legacyId":139},{"title":112,"slug":109,"images":113,"description":116,"details":117,"durée":118,"role":122,"technos":123,"team":130},"Fiches pratiques environnementales pour la fonction publique",[114,115],"/projetFP/archiFP.png","/projetFP/ficheRH.png","Un projet pilote conçu pour accompagner les agents de la fonction publique dans l'intégration de pratiques plus respectueuses de l'environnement à travers des fiches pratiques accessibles. L'objectif est de fournir un support concret, évolutif, et co-construit par les utilisateurs, dans une logique de sobriété numérique.\n","Après une phase d'exploration des outils existants (Suite Numérique, Tchap), nous avons opté pour la mise en place d’un wiki frugal, centré sur la production collaborative de fiches pratiques.\n\nLe cœur du dispositif repose sur deux systèmes RAG (Retrieval-Augmented Generation) expérimentés en parallèle, avec des contenus provenant d'une API développée avec WeLearn, de la littérature grise, et de contributions internes. Les utilisateurs peuvent enrichir les fiches, proposer des modifications, et manifester leur intérêt via un formulaire.\n\nEn cas de convergence sur une thématique, un système de proposition automatisée de visio est déclenché pour favoriser l’échange collectif. Un retraining progressif permet d’améliorer la pertinence des fiches au fil du temps.\n",[119,120,121],"Phase 1 (2 mois) Compréhension et analyse des besoins.","Phase 2 (4 mois) Choix de la stack technologique et amélioration du modèle.","Phase 3 (1 mois) Déploiement et automatisation de la solution.","Responsable technique du POC, intégration des systèmes RAG, API WeLearn et mise en place du pipeline d’amélioration continue",[124,125,126,29,32,127,128,129],"Wiki frugal (statique)","RAG (x2 moteurs comparés)","API WeLearn","Automatisation email/visio","UX Research (entretiens utilisateurs)","UI Design collaboratif","Projet mené en collaboration avec des responsables RH de la fonction publique issus de plusieurs territoires, des experts en UX/UI, et l'équipe technique de WeLearn. L’approche a été fortement centrée utilisateur, avec des ateliers et des tests continus.\n","src/content/projets/ecoskills.md","7caf27942ff16bef",{"html":43,"metadata":134},{"headings":135,"localImagePaths":136,"remoteImagePaths":137,"frontmatter":111,"imagePaths":138},[],[],[],[],"ecoskills.md","detection-defaillance-photovoltaique",{"id":140,"data":142,"filePath":162,"digest":163,"rendered":164,"legacyId":170},{"title":143,"slug":140,"images":144,"description":149,"details":150,"durée":151,"role":155,"technos":156,"team":161},"Système de détection de défaillance sur fermes photovoltaïques flottantes",[145,146,147,148],"/projetPV/analyse.png","/projetPV/pv2.png","/projetPV/roadmap.png","/projetPV/pvschema.png","Une solution innovante de détection automatique de défaillances sur des fermes de panneaux photovoltaïques flottants sur des lacs. Le système combine IoT, analyse de données et machine learning pour optimiser la maintenance et maximiser la production.\n","Ce projet a débuté par une analyse approfondie des données collectées via des capteurs IoT installés sur la centrale EDF. Avec une équipe composée d’un développeur et de deux ingénieurs, nous avons mis en place des méthodes de data science pour identifier et optimiser les informations pertinentes pour la détection d’anomalies.\n\nLa réflexion s’est concentrée sur l’obtention des insights les plus fiables avec le meilleur rapport qualité/prix. Ensuite, une solution automatisée a été développée, incluant un modèle de machine learning déployé pour détecter en temps réel les défaillances.\n\nLe déploiement a été effectué en interne avec une architecture robuste utilisant Terraform pour l’infrastructure, Airflow pour l’orchestration des workflows, et PostgreSQL pour la gestion des données.\n",[152,153,154],"Phase 1 (6 mois) Analyse des besoins et étude de la viabilité du modèle de détection d’anomalies.","Phase 2 (en cours) Déploiement des capteurs IoT sur site.","Phase 3 (à venir) Entraînement et déploiement du modèle de détection, avec mise en place de l’automatisation.","Data Scientist et MLOps, responsable des modèles ML et du déploiement automatisé",[29,157,35,158,34,159,32,160],"Prometheus","Clustering","Terraform","IoT","Projet mené avec un développeur et deux ingénieurs, en forte collaboration autour de la collecte IoT, du développement ML et de l’intégration opérationnelle sur la centrale EDF.\n","src/content/projets/helioslite.md","840588696cdd54c7",{"html":43,"metadata":165},{"headings":166,"localImagePaths":167,"remoteImagePaths":168,"frontmatter":142,"imagePaths":169},[],[],[],[],"helioslite.md","projets-educatifs-solidaires",{"id":171,"data":173,"filePath":198,"digest":199,"rendered":200,"legacyId":206},{"title":174,"slug":171,"images":175,"description":180,"details":181,"role":182,"durée":183,"technos":188,"team":197},"Applications solidaires et éducatives pour l'Éducation Nationale, les assistant·es maternel·les et les PME",[176,177,178,179],"/projet+/nounou.png","/projet+/dys.png","/projet+/chatbot.png","/projet+/pme.png","Une série d’applications légères mais utiles, conçues pour répondre à des besoins concrets du quotidien dans les domaines de l’éducation, de la petite enfance, et de la gestion PME. Des solutions simples, accessibles, et pensées pour être partagées.\n","Ces projets ont été réalisés de manière autonome avec une approche orientée impact social, souvent en collaboration informelle avec des professionnel·les de terrain. L’objectif : proposer des outils numériques concrets et gratuits à des utilisateurs parfois éloignés du numérique. \n\n **Application de suivi pour assistantes maternelles**: développée avec Streamlit, cette app permet un suivi quotidien des enfants (repas, soins, siestes), un espace partagé avec les parents (photos, notes, documents), ainsi qu’un système d’authentification pour garantir la confidentialité. \n\n**Outil d’accessibilité pour élèves à besoins particuliers** : cette webapp permet de transformer des fichiers PDF en version audio, de traduire automatiquement des documents dans plusieurs langues, ou encore de convertir de l’audio vers du texte, pour s’adapter à différents handicaps ou situations linguistiques.\n\n**Chatbot éducatif avec système RAG (Retrieval-Augmented Generation)** : conçu pour répondre aux préoccupations des parents d’élèves à besoins particuliers, il propose des réponses personnalisées, des conseils pratiques et des ressources officielles.\n\n**Application de gestion documentaire pour PME** : permet de générer et organiser facilement des documents types (contrats, devis, relances) avec des modèles préformatés et une interface simple, pensée pour les très petites entreprises.\n","Développeur full-stack indépendant, en charge de toutes les étapes (idée, développement, design, déploiement). Projets réalisés par passion pour résoudre des problèmes concrets et les partager librement.",[184,185,186,187],"Projet nounou – 2 mois : Conception d'une application Streamlit de suivi des enfants, en lien avec plusieurs assistantes maternelles pour définir les besoins terrain et concevoir une interface simple et sécurisée.","Outil accessibilité – 1,5 mois : Développement d’une webapp pour convertir, traduire et adapter des documents à différents handicaps ou contextes linguistiques, en collaboration avec un enseignant référent.","Chatbot éducatif – 2 mois : Création d’un assistant intelligent avec RAG, capable de répondre aux questions de parents d’enfants à besoins particuliers, incluant un corpus de ressources officielles.","Gestion documentaire PME – 1,5 mois : Réalisation d’une application interne no-code/low-code pour générer et gérer automatiquement des documents types, adaptée aux TPE avec peu de moyens techniques.",[29,189,190,96,191,192,193,194,195,196],"Streamlit","LangChain","HTML/CSS","JavaScript","Whisper / TTS","PDF parsing","Hugging Face Transformers","SQLite / PostgreSQL","Projets personnels menés en solo, avec des retours réguliers de professionnel·les du secteur (enseignant·es, assistant·es maternel·les, dirigeants de PME) pour ajuster les fonctionnalités aux usages réels.\n","src/content/projets/projets+.md","972a2bb7bdbc7430",{"html":43,"metadata":201},{"headings":202,"localImagePaths":203,"remoteImagePaths":204,"frontmatter":173,"imagePaths":205},[],[],[],[],"projets+.md","classification-clients-huissiers",{"id":207,"data":209,"filePath":228,"digest":229,"rendered":230,"legacyId":236},{"title":210,"slug":207,"images":211,"description":214,"details":215,"durée":216,"role":221,"technos":222,"team":227},"Classification de clients pour optimisation des relances téléphoniques",[212,213],"/huissiers/archi.png","/huissiers/bdd.png","Projet de data science et machine learning réalisé pour un groupe d’huissiers afin de prédire la probabilité de remboursement de créanciers. L’objectif était d’optimiser les appels de relance en priorisant les clients selon leur potentiel de paiement.\n","Le projet a débuté par une analyse exploratoire approfondie des bases de données clients transmises par les huissiers. Cette étape a permis de comprendre la structure, la variabilité et la véracité des informations disponibles. Un travail rigoureux de nettoyage a été effectué, notamment via la mise en place d’un pipeline ELT pour fiabiliser les données entrantes.\nEn collaboration avec l’équipe SI (2 personnes) et l’équipe administrative en charge des appels, nous avons identifié les critères influençant la capacité de remboursement. Sur cette base, plusieurs itérations de modèles de classification ont été entraînées et évaluées, en intégrant progressivement le retour terrain pour affiner les prédictions.\nUn système de scoring a été mis en place pour prioriser les relances, avec des tests réguliers d’amélioration des performances. Enfin, la solution a été déployée et automatisée, permettant une classification continue et une meilleure efficacité des équipes de relance.\n",[217,218,219,220],"Phase 1 (2 mois) : Analyse des tables, vérification de la qualité des données et mise en place d’un pipeline ELT pour le nettoyage initial.","Phase 2 (2 mois) : Collaboration avec l’équipe métier pour identifier les critères de scoring, exploration statistique et construction des premières features.","Phase 3 (2 mois) : Entraînement et amélioration de modèles de classification, tests croisés avec retours de l’équipe administrative.","Phase 4 (2 mois) : Déploiement du modèle, automatisation des flux de données et intégration dans le système de relance.","Arhictecte data, responsable du pipeline ELT, de la modélisation prédictive et de l’automatisation de la solution",[29,223,224,34,32,96,225,226],"Scikit-learn","Pandas","ELT","Classification supervisée","Projet mené avec l’équipe SI (2 personnes) et l’équipe administrative des huissiers. Collaboration étroite pour le recueil des besoins métiers, la validation des résultats et l’intégration dans le processus opérationnel de relance téléphonique.\n","src/content/projets/huissiers.md","98f74b1cab2b48d0",{"html":43,"metadata":231},{"headings":232,"localImagePaths":233,"remoteImagePaths":234,"frontmatter":209,"imagePaths":235},[],[],[],[],"huissiers.md","classification-retours-techniques",{"id":237,"data":239,"filePath":258,"digest":259,"rendered":260,"legacyId":266},{"title":240,"slug":237,"images":241,"description":244,"details":245,"durée":246,"role":251,"technos":252,"team":257},"Classification intelligente des retours techniques sur distributeurs de médicaments",[242,243],"/projetRetours/architecture.png","/projetRetours/dashboard.png","Mise en place d’un système de classification automatique des retours techniques issus des distributeurs automatiques de médicaments. Grâce à des techniques de machine learning non supervisé et à un dashboard Power BI, l’entreprise peut désormais prioriser ses interventions et améliorer sa maintenance préventive.\n","L’entreprise installe et maintient des machines de distribution de médicaments en pharmacie. Face à une volumétrie croissante de retours techniques transmis par les techniciens, l’objectif a été de structurer ces données pour en extraire de la valeur.\nLe projet a commencé par une collaboration étroite avec l’équipe technique pour définir les grandes **catégories de pannes** et **mots-clés caractéristiques**. Ces éléments ont ensuite guidé une phase de **clustering non supervisé** en deep learning sur des retours non labellisés, permettant de faire émerger des regroupements cohérents.\nUne fois la classification validée et enrichie en itération avec l’équipe technique, elle a été **déployée directement dans la base de données** centralisant les incidents. Enfin, un **dashboard interactif sous Power BI** a été développé pour permettre à la direction et aux équipes opérationnelles de visualiser les typologies de pannes, les fréquences, et les tendances par pharmacie, par machine, ou par période.\n",[247,248,249,250],"Phase 1 (1 mois) Définition des catégories et mots-clés avec l’équipe technique","Phase 2 (1,5 mois) Clustering non supervisé sur les tickets techniques","Phase 3 (1 mois) Déploiement de la classification sur la base de données","Phase 4 (0,5 mois) Développement et mise en production du dashboard Power BI","Architecte Data  – responsable du traitement NLP, du clustering et du reporting analytique",[29,253,254,255,256,32],"scikit-learn","TensorFlow","NLP","Power BI","Projet mené en collaboration avec l’équipe technique (techniciens de maintenance), l’équipe administrative et la direction de l’entreprise. Une approche collaborative a permis d’aligner les catégories métier avec les données réelles et de favoriser l’adoption des outils analytiques.\n","src/content/projets/synergie.md","575f7a39068ef7ca",{"html":43,"metadata":261},{"headings":262,"localImagePaths":263,"remoteImagePaths":264,"frontmatter":239,"imagePaths":265},[],[],[],[],"synergie.md"]